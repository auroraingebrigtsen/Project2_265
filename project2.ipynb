{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 265\n",
    "torch.manual_seed(SEED)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Object Localization\n",
    "#### First we load and inspect the localization_*** datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('data/localization_train.pt')\n",
    "val_data = torch.load('data/localization_val.pt')\n",
    "test_data = torch.load('data/localization_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 59400\n",
      "Val data size: 6600\n",
      "Test data size: 11000\n"
     ]
    }
   ],
   "source": [
    "print(f'Train data size: {len(train_data)}')\n",
    "print(f'Val data size: {len(val_data)}')\n",
    "print(f'Test data size: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first image: torch.Size([1, 48, 60])\n",
      "Type of first image: <class 'torch.Tensor'>\n",
      "\n",
      "Shape of first label: torch.Size([6])\n",
      "Type of first label: <class 'torch.Tensor'>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.6000, 0.2292, 0.3667, 0.4167, 4.0000], dtype=torch.float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_img, first_label = train_data[0]\n",
    "\n",
    "print(f'Shape of first image: {first_img.shape}')\n",
    "print(f'Type of first image: {type(first_img)}')\n",
    "\n",
    "print(f'\\nShape of first label: {first_label.shape}')\n",
    "print(f'Type of first label: {type(first_label)})')\n",
    "first_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in Training Data\n",
      "0: 5345\n",
      "1: 11475\n",
      "2: 5365\n",
      "3: 5522\n",
      "4: 5243\n",
      "5: 4889\n",
      "6: 5310\n",
      "7: 5644\n",
      "8: 5249\n",
      "9: 5358\n",
      "Class distribution in Validation Data\n",
      "0: 578\n",
      "1: 1267\n",
      "2: 593\n",
      "3: 609\n",
      "4: 599\n",
      "5: 532\n",
      "6: 608\n",
      "7: 621\n",
      "8: 602\n",
      "9: 591\n",
      "Class distribution in Test Data\n",
      "0: 980\n",
      "1: 2135\n",
      "2: 1032\n",
      "3: 1010\n",
      "4: 982\n",
      "5: 892\n",
      "6: 958\n",
      "7: 1028\n",
      "8: 974\n",
      "9: 1009\n"
     ]
    }
   ],
   "source": [
    "def count_instances(data, data_name=None) -> None:\n",
    "    \"\"\"Counts the number of instances of each class in a dataset\"\"\"\n",
    "    counter = Counter([int(label[-1]) for _, label in data])\n",
    "    sorted_counter = dict(sorted(counter.items()))\n",
    "    if data_name is not None:\n",
    "        print(f'Class distribution in {data_name}')\n",
    "    for key, value in sorted_counter.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "count_instances(train_data, 'Training Data')\n",
    "count_instances(val_data, 'Validation Data')\n",
    "count_instances(test_data, 'Test Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 48, 60])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape\n",
    "#hÃ¸yde bredde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting one image from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13, 30, 18, 24], dtype=torch.uint8)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m bbox \u001b[38;5;241m=\u001b[39m bbox\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(bbox)\n\u001b[1;32m---> 13\u001b[0m img_with_bbox \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m img_with_bbox  \u001b[38;5;241m=\u001b[39m img_with_bbox\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#img = img.numpy().transpose((1, 2, 0))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aurora Ingebrigtsen\\anaconda3\\envs\\INF265\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aurora Ingebrigtsen\\anaconda3\\envs\\INF265\\lib\\site-packages\\torchvision\\utils.py:201\u001b[0m, in \u001b[0;36mdraw_bounding_boxes\u001b[1;34m(image, boxes, labels, colors, fill, width, font, font_size)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m image\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m}:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly grayscale and RGB images are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[43mboxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m boxes[:, \u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m (boxes[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m boxes[:, \u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoxes need to be in (xmin, ymin, xmax, ymax) format. Use torchvision.ops.box_convert to convert them\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    206\u001b[0m num_boxes \u001b[38;5;241m=\u001b[39m boxes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(8,3))\n",
    "\n",
    "for i, ax in enumerate(axes.flat): \n",
    "    img, bbox_normalized = next((img, label[1:5]) for img, label in train_data if int(label[-1]) == i)\n",
    "    img_height, img_width = train_data[0][0].shape[-2], train_data[0][0].shape[-1]\n",
    "    bbox = bbox_normalized.clone()\n",
    "    bbox[0] *= img_width # xmin\n",
    "    bbox[1] *= img_height # ymin\n",
    "    bbox[2] *= img_width  # xmax\n",
    "    bbox[3] *= img_height  # ymax\n",
    "    bbox = bbox.type(torch.uint8)\n",
    "    print(bbox)\n",
    "    img_with_bbox = draw_bounding_boxes(img.type(torch.uint8), bbox)\n",
    "    img_with_bbox  = img_with_bbox.numpy().transpose((1, 2, 0))\n",
    "    #img = img.numpy().transpose((1, 2, 0))\n",
    "    ax.imshow(img_with_bbox, cmap='gray')\n",
    "    ax.set_title(i)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class(data:torch.tensor, class_label:int, start_idx:int=0) -> None:\n",
    "    \"\"\"Plots a subplot with 10 images from a given class, starting at a chosen index\n",
    "    Parameters:\n",
    "    data: torch.tensor\n",
    "        Data containing images and labels\n",
    "    class_label: int\n",
    "        The class to plot images from\n",
    "    start_idx: int\n",
    "        The index of the first image to be plotted. If start_idx=n, the subplot will contain \n",
    "        the n'th to the n+10'th image from the class\n",
    "    Returns:\n",
    "    None\"\"\"\n",
    "    class_images = [img for img, label in data if int(label[-1]) == class_label]\n",
    "    _, axes = plt.subplots(nrows=2, ncols=5, figsize=(8,3))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        idx = start_idx + i\n",
    "        img = class_images[idx]\n",
    "        img = img.numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        plt.suptitle(f'CLASS {class_label} - Image {start_idx} to {idx}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_class(train_data, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a normalizer and a preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.stack([img for img, _ in train_data])\n",
    "\n",
    "# Define normalizer\n",
    "normalizer_pipe = transforms.Normalize(\n",
    "    imgs.mean(dim=(0, 2, 3)), \n",
    "    imgs.std(dim=(0, 2, 3))\n",
    "    )\n",
    "\n",
    "# Definer preprocessor including the normalizer\n",
    "preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalizer_pipe\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('data/localization_train.pt')\n",
    "val_data = torch.load('data/localization_val.pt')\n",
    "test_data = torch.load('data/localization_test.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
