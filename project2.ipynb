{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.ops import box_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 265\n",
    "torch.manual_seed(SEED)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Object Localization\n",
    "#### First we load and inspect the localization_*** datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('data/localization_train.pt')\n",
    "val_data = torch.load('data/localization_val.pt')\n",
    "test_data = torch.load('data/localization_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train data size: {len(train_data)}')\n",
    "print(f'Val data size: {len(val_data)}')\n",
    "print(f'Test data size: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_img, first_label = train_data[0]\n",
    "\n",
    "print(f'Shape of first image: {first_img.shape}')\n",
    "print(f'Type of first image: {type(first_img)}')\n",
    "\n",
    "print(f'\\nShape of first label: {first_label.shape}')\n",
    "print(f'Type of first label: {type(first_label)})')\n",
    "first_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_instances(data, data_name=None) -> None:\n",
    "    \"\"\"Counts the number of instances of each class in a dataset\"\"\"\n",
    "    counter = Counter([int(label[-1]) for _, label in data])\n",
    "    sorted_counter = dict(sorted(counter.items()))\n",
    "    if data_name is not None:\n",
    "        print(f'Class distribution in {data_name}')\n",
    "    for key, value in sorted_counter.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "count_instances(train_data, 'Training Data')\n",
    "count_instances(val_data, 'Validation Data')\n",
    "count_instances(test_data, 'Test Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][0].shape\n",
    "#hÃ¸yde bredde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_label[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting one image from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(8,3))\n",
    "\n",
    "for i, ax in enumerate(axes.flat): \n",
    "    img, bbox = next((img, label[1:5]) for img, label in train_data if int(label[-1]) == i)\n",
    "    img_height, img_width = train_data[0][0].shape[-2], train_data[0][0].shape[-1]\n",
    "\n",
    "    img = (img * 255).byte()\n",
    "\n",
    "    bbox[0] *= img_width\n",
    "    bbox[1] *= img_height\n",
    "    bbox[2] *= img_width\n",
    "    bbox[3] *= img_height\n",
    "\n",
    "    bbox = bbox.type(torch.uint8)\n",
    "\n",
    "    converted_bbox = box_convert(bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "\n",
    "    img_with_bbox = draw_bounding_boxes(img, converted_bbox.unsqueeze(0), colors='red')\n",
    "    img_with_bbox  = img_with_bbox.numpy().transpose((1, 2, 0))\n",
    "    ax.imshow(img_with_bbox, cmap='gray')\n",
    "    ax.set_title(i)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class(data:torch.tensor, class_label:int, start_idx:int=0) -> None:\n",
    "    \"\"\"Plots a subplot with 10 images from a given class, starting at a chosen index\"\"\"\n",
    "    class_images = [img for img, label in data if int(label[-1]) == class_label]\n",
    "    bboxes = [label[1:5] for img, label in data if int(label[-1]) == class_label]\n",
    "    _, axes = plt.subplots(nrows=2, ncols=5, figsize=(8,3))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "\n",
    "        idx = start_idx + i\n",
    "        img = class_images[idx]\n",
    "        bbox = bboxes[idx]\n",
    "\n",
    "        img_height, img_width = train_data[0][0].shape[-2], train_data[0][0].shape[-1]\n",
    "\n",
    "        img = (img * 255).byte()\n",
    "\n",
    "        bbox[0] *= img_width\n",
    "        bbox[1] *= img_height\n",
    "        bbox[2] *= img_width\n",
    "        bbox[3] *= img_height\n",
    "\n",
    "        bbox = bbox.type(torch.uint8)\n",
    "\n",
    "        converted_bbox = box_convert(bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "\n",
    "        img_with_bbox = draw_bounding_boxes(img, converted_bbox.unsqueeze(0), colors='red')\n",
    "        img_with_bbox  = img_with_bbox.numpy().transpose((1, 2, 0))\n",
    "        ax.imshow(img_with_bbox, cmap='gray')\n",
    "        plt.suptitle(f'CLASS {class_label} - Image {start_idx} to {idx}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_class(train_data, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a normalizer and a preprocessor TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.stack([img for img, _ in train_data])\n",
    "\n",
    "# Define normalizer\n",
    "normalizer_pipe = transforms.Normalize(\n",
    "    imgs.mean(dim=(0, 2, 3)), \n",
    "    imgs.std(dim=(0, 2, 3))\n",
    "    )\n",
    "\n",
    "# Definer preprocessor including the normalizer\n",
    "preprocessor = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            normalizer_pipe\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load('data/localization_train.pt')\n",
    "val_data = torch.load('data/localization_val.pt')\n",
    "test_data = torch.load('data/localization_test.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFn(nn.Module):\n",
    "    \"\"\"Custom loss function\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_a = nn.BCEWithLogitsLoss()  # detection loss\n",
    "        self.L_b = nn.MSELoss()  # localization loss\n",
    "        self.L_c = nn.CrossEntropyLoss()  # classification loss\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        #print(f'Ypred{y_pred}\\nYtrue{y_true}\\nYpred[0]{y_pred[0]}')\n",
    "        L_a = self.L_a(y_pred[0][0], y_true[0][0])\n",
    "        \n",
    "        if y_pred[0][0] <= 0.5:\n",
    "            return L_a\n",
    "\n",
    "        L_b = self.L_b(y_pred[0][1:5], y_true[0][1:5])\n",
    "        L_c = self.L_c(y_pred[0][5:], y_true[0][-1].long())\n",
    "\n",
    "        return L_a + L_b + L_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, device=device, dtype=torch.double)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1, device=device, dtype=torch.double)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, device=device, dtype=torch.double)\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(12*15*64, 15, device=device)  # Adjust input size based on your input image size\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with activation functions\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2, stride=2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2, stride=2)\n",
    "        out = F.relu(self.conv3(out))\n",
    "        #out = F.max_pool2d(out, 2)\n",
    "        # Flatten the output from convolutional layers\n",
    "        out = self.flat(out)\n",
    "        # Apply fully connected layers with activation functions\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=1, shuffle=False)\n",
    "\n",
    "model = MyCNN()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = LossFn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device, dtype=torch.double)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(datetime.now().time(), epoch, loss_train / n_batch))\n",
    "\n",
    "            \n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train(\n",
    "    n_epochs=10,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
