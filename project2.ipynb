{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.ops import box_convert, box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 265\n",
    "torch.manual_seed(SEED)\n",
    "torch.set_default_dtype(torch.double)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Object Localization\n",
    "#### First we load and inspect the localization datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_train = torch.load('data/localization_train.pt')\n",
    "loc_val = torch.load('data/localization_val.pt')\n",
    "loc_test = torch.load('data/localization_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train data size: {len(loc_train)}')\n",
    "print(f'Val data size: {len(loc_val)}')\n",
    "print(f'Test data size: {len(loc_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_img, first_label = loc_train[0]\n",
    "\n",
    "print(f'Shape of first image: {first_img.shape}')\n",
    "print(f'Type of first image: {type(first_img)}')\n",
    "\n",
    "print(f'\\nShape of first label: {first_label.shape}')\n",
    "print(f'Type of first label: {type(first_label)})')\n",
    "first_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_instances(data, data_name=None) -> None:\n",
    "    \"\"\"Counts the number of instances of each class in a dataset\"\"\"\n",
    "    counter = Counter([99 if label[0] == 0 else int(label[-1]) for _, label in data])\n",
    "    sorted_counter = dict(sorted(counter.items()))\n",
    "    if data_name is not None:\n",
    "        print(f'Class distribution in {data_name}')\n",
    "    for key, value in sorted_counter.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "# Assuming train_data, val_data, and test_data are defined elsewhere\n",
    "count_instances(loc_train, 'Training Data')\n",
    "count_instances(loc_val, 'Validation Data')\n",
    "count_instances(loc_test, 'Test Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting one image from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(data):\n",
    "    _, axes = plt.subplots(nrows=2, ncols=6, figsize=(8,3))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat): \n",
    "\n",
    "        if i == 10:\n",
    "            img = next(img for img, label in data if int(label[0]) == 0)\n",
    "            img = img.numpy().transpose((1, 2, 0))\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.set_title('None')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        if i == 11:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        img, bbox = next((img, label[1:5]) for img, label in data if int(label[-1]) == i)\n",
    "        img_height, img_width = img.shape[-2], img.shape[-1]\n",
    "\n",
    "        img = (img * 255).byte()\n",
    "\n",
    "        bbox[0] *= img_width\n",
    "        bbox[1] *= img_height\n",
    "        bbox[2] *= img_width\n",
    "        bbox[3] *= img_height\n",
    "\n",
    "        bbox = bbox.type(torch.uint8)\n",
    "\n",
    "        converted_bbox = box_convert(bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "\n",
    "        img_with_bbox = draw_bounding_boxes(img, converted_bbox.unsqueeze(0), colors='red')\n",
    "        img_with_bbox  = img_with_bbox.numpy().transpose((1, 2, 0))\n",
    "        ax.imshow(img_with_bbox, cmap='gray')\n",
    "        ax.set_title(i)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class(data:torch.tensor, class_label:int, start_idx:int=0) -> None:\n",
    "    \"\"\"Plots a subplot with 10 images from a given class, starting at a chosen index\"\"\"\n",
    "    class_images = [img for img, label in data if int(label[-1]) == class_label]\n",
    "    bboxes = [label[1:5] for img, label in data if int(label[-1]) == class_label]\n",
    "    _, axes = plt.subplots(nrows=2, ncols=5, figsize=(8,3))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "\n",
    "        idx = start_idx + i\n",
    "        img = class_images[idx]\n",
    "        bbox = bboxes[idx]\n",
    "\n",
    "        img_height, img_width = data[0][0].shape[-2], data[0][0].shape[-1]\n",
    "\n",
    "        img = (img * 255).byte()\n",
    "\n",
    "        bbox[0] *= img_width\n",
    "        bbox[1] *= img_height\n",
    "        bbox[2] *= img_width\n",
    "        bbox[3] *= img_height\n",
    "\n",
    "        bbox = bbox.type(torch.uint8)\n",
    "\n",
    "        converted_bbox = box_convert(bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "\n",
    "        img_with_bbox = draw_bounding_boxes(img, converted_bbox.unsqueeze(0), colors='lightgreen')\n",
    "        img_with_bbox  = img_with_bbox.numpy().transpose((1, 2, 0))\n",
    "        ax.imshow(img_with_bbox, cmap='gray')\n",
    "        plt.suptitle(f'CLASS {class_label} - Image {start_idx} to {idx}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_class(loc_train, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a normalizer and a preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.stack([img for img, _ in loc_train])\n",
    "\n",
    "# Define normalizer\n",
    "normalizer_pipe = transforms.Normalize(\n",
    "    imgs.mean(dim=(0, 2, 3)), \n",
    "    imgs.std(dim=(0, 2, 3))\n",
    "    )\n",
    "\n",
    "# Define preprocessor including the normalizer\n",
    "preprocessor = transforms.Compose([\n",
    "            normalizer_pipe\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_train = [(preprocessor(img), label) for img, label in loc_train]\n",
    "loc_val = [(preprocessor(img), label) for img, label in loc_val]\n",
    "loc_test = [(preprocessor(img), label) for img, label in loc_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalizationLoss(nn.Module):\n",
    "    \"\"\"Custom loss function\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_a = nn.BCEWithLogitsLoss()  # detection loss\n",
    "        self.L_b = nn.MSELoss()  # localization loss\n",
    "        self.L_c = nn.CrossEntropyLoss()  # classification loss\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        det_pred = y_pred[:, 0]\n",
    "        bbox_pred = y_pred[:, 1:5]\n",
    "        class_pred = y_pred[:, 5:]\n",
    "\n",
    "        det_true = y_true[:, 0]\n",
    "        bbox_true = y_true[:, 1:5]\n",
    "        class_true = y_true[:, -1].long()\n",
    "\n",
    "        L_a = self.L_a(det_pred, det_true)\n",
    "\n",
    "        object_detected = det_true == 1\n",
    "\n",
    "        L_b = self.L_b(bbox_pred[object_detected], bbox_true[object_detected])\n",
    "        L_c = self.L_c(class_pred[object_detected], class_true[object_detected])\n",
    "\n",
    "        return L_a + L_b + L_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to compute size of fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_size(input_size, layer):\n",
    "    H_in = input_size[0]\n",
    "    W_in = input_size[1]\n",
    "    C_in = layer.in_channels\n",
    "    C_out = layer.out_channels\n",
    "    kernel_size = layer.kernel_size\n",
    "    padding = layer.padding\n",
    "    stride = layer.stride\n",
    "\n",
    "    H_out = (H_in+2*padding[0]-kernel_size[0])/stride[0]\n",
    "    W_out = (W_in+2*padding[1]-kernel_size[1])/stride[1]\n",
    "\n",
    "    return H_out * W_out * C_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, device=device, dtype=torch.double)\n",
    "conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1, device=device, dtype=torch.double)\n",
    "conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, device=device, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_pipe(layers, input_size, num_outputs):\n",
    "    output_size = input_size\n",
    "    for layer in layers:\n",
    "        output_size = get_output_size(output_size, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, device=device, dtype=torch.double)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1, device=device, dtype=torch.double)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, device=device, dtype=torch.double)\n",
    "        self.fc1 = nn.Linear(12*15*64, 15, device=device, dtype=torch.double)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2, stride=2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2, stride=2)\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(model, loader):\n",
    "    '''\n",
    "    Function that uses a model to predict and calculate accuracy\n",
    "    '''\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    iou_sum = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=device, dtype=torch.double)\n",
    "            labels = labels.to(device=device, dtype=torch.double)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            det_pred = F.sigmoid(outputs[:, 0])\n",
    "            object_detected = det_pred > 0.5\n",
    "\n",
    "            _, class_pred = torch.max(outputs[:, 5:], dim=1)\n",
    "\n",
    "            det_true = labels[:, 0].int()\n",
    "            class_true = labels[:, -1].int()\n",
    "\n",
    "            total += labels.shape[0]\n",
    "            correct += ((object_detected == 0) & (det_true == 0)).sum()\n",
    "            correct += ((object_detected == 1) & (det_true == 1) & (class_pred == class_true)).sum()\n",
    "\n",
    "            bbox_pred = outputs[:, 1:5]\n",
    "            bbox_true = labels[:, 1:5]\n",
    "\n",
    "            iou_sum += box_iou(bbox_pred[object_detected], bbox_true[object_detected]).sum()\n",
    "\n",
    "    acc =  correct / total\n",
    "    iou = iou_sum / total\n",
    "\n",
    "    performance = (acc + iou) / 2\n",
    "    \n",
    "    return acc, iou, performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to plot training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss:list, val_loss:list, title:str) -> None:\n",
    "    \"\"\"Plots the training and validation loss\"\"\"\n",
    "    _, ax = plt.subplots()\n",
    "    ax.plot(np.arange(1,len(train_loss)+1), train_loss, label='Training loss')\n",
    "    ax.plot(np.arange(1,len(val_loss)+1), val_loss, label='Validation loss')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, val_loader):\n",
    "    \n",
    "    n_batch_train = len(train_loader)\n",
    "    n_batch_val = len(val_loader)\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        loss_train = 0\n",
    "        loss_val = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for imgs, labels in train_loader:\n",
    "\n",
    "            imgs = imgs.to(device=device, dtype=torch.double)\n",
    "            labels = labels.to(device=device, dtype=torch.double)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        model.eval()\n",
    "\n",
    "        with torch.inference_mode(): # <-- Equivalent to no_grad, if no error is provided this is preferred.\n",
    "            for imgs, labels in val_loader:\n",
    "\n",
    "                imgs = imgs.to(device=device, dtype=torch.double)\n",
    "                labels = labels.to(device=device, dtype=torch.double)\n",
    "\n",
    "                outputs = model(imgs)\n",
    "\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss_val += loss.item()\n",
    "            \n",
    "        losses_train.append(loss_train / n_batch_train)\n",
    "        losses_val.append(loss_val / n_batch_val)\n",
    "\n",
    "        #if epoch == 1 or epoch % 10 == 0:\n",
    "        print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(datetime.now().strftime('%H:%M:%S'), epoch, loss_train / n_batch_train))\n",
    "        print('{}  |  Epoch {}  |  Validation loss {:.3f}'.format(datetime.now().strftime('%H:%M:%S'), epoch, loss_val / n_batch_val))\n",
    "\n",
    "    train_acc, train_iou, train_performance = compute_performance(model, train_loader)\n",
    "    val_acc, val_iou, val_performance = compute_performance(model, val_loader)\n",
    "    print(f'Training performance: Accuracy = {train_acc}, IOU = {train_iou}, Overall = {train_performance}')\n",
    "    print(f'Training performance: Accuracy = {val_acc}, IOU = {val_iou}, Overall = {val_performance}')\n",
    "\n",
    "    return losses_train, losses_val, train_performance, val_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(loc_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(loc_val, batch_size=64, shuffle=False)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "model = MyCNN()\n",
    "model.to(device=device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0, weight_decay=0)\n",
    "loss_fn = LocalizationLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train, loss_val, train_perform, val_perform = train(\n",
    "    n_epochs=5,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader\n",
    ")\n",
    "\n",
    "plot_loss(loss_train, loss_val, 'Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the best model TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selector(models:list, performances:list):\n",
    "    \"\"\"Given a list of models, returns the model that has best accuracy score on validation data\"\"\"\n",
    "    best_model = None\n",
    "    best_performance = 0\n",
    "\n",
    "    for idx, model in enumerate(models):\n",
    "        if performances[idx] > best_performance:\n",
    "            best_model = model\n",
    "            best_performance = performances[idx]\n",
    "\n",
    "    return best_model, best_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_performance = model_selector([model], [performance])\n",
    "\n",
    "# Print additional details of the best model\n",
    "print(\"Best Model Details\\n--------------------------------------------------------------\")\n",
    "print(f\"Network architecture/ layout: {best_model}\\n\")\n",
    "#print(f\"Optimizer Parameters: {best_data.optimizer\")\n",
    "print(f\"Validation Performance: {best_performance}\")\n",
    "#print(f\"Validation Accuracy {round(best_data['model_man_val_accuracy'], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the best model on unseen data TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(loc_test, batch_size=64, shuffle=False)\n",
    "\n",
    "test_acc, test_iou, test_performance = compute_performance(best_model, test_loader)\n",
    "print(10*'-'+'Test Performance' + 10*'-')\n",
    "print(f\"Test Accuracy: {test_acc}\\nTest IOU: {test_iou}\\nOverall Performance: {test_performance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    '''\n",
    "    Function that creates a y and y_pred tensor given a model and a loader\n",
    "    '''\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = torch.empty(0, device=device)\n",
    "    y_pred = torch.empty(0, device=device)\n",
    "\n",
    "    with torch.inference_mode(): # <-- Equivalent to no_grad, if no error is provided this is preferred.\n",
    "        for imgs, labels in loader:\n",
    "            \n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            _, class_pred = torch.max(outputs[:, 5:], dim=1)\n",
    "\n",
    "            predicted = torch.cat((outputs[:, :5], class_pred.unsqueeze(1)), dim=1)\n",
    "            \n",
    "            y_true = torch.cat((y_true, labels), dim=0)\n",
    "            y_pred = torch.cat((y_pred, predicted.data), dim=0)\n",
    "                \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(imgs, y_true:torch.tensor, y_pred:torch.tensor, label:int=0, start_idx:int=0) -> None:\n",
    "    \"\"\"Plots things\"\"\"\n",
    "    class_mask = y_true[:, -1] == label\n",
    "    class_imgs = [img for idx, img in enumerate(imgs) if class_mask[idx]]\n",
    "    class_true, class_pred = y_true[class_mask], y_pred[class_mask]\n",
    "    \n",
    "    true_bboxes = [label[1:5] for label in class_true]\n",
    "    pred_bboxes = [label[1:5] for label in class_pred]\n",
    "\n",
    "    _, axes = plt.subplots(nrows=2, ncols=5, figsize=(8,3))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "\n",
    "        idx = start_idx + i\n",
    "        img = class_imgs[idx]\n",
    "\n",
    "        img_height, img_width = img.shape[-2], img.shape[-1]\n",
    "        img = (img * 255).byte()\n",
    "\n",
    "        if int(class_true[idx][0]) == 1:\n",
    "            true_bbox = true_bboxes[idx] # TODO repetiv kode, lage en funksjon\n",
    "            true_bbox[0] *= img_width\n",
    "            true_bbox[1] *= img_height\n",
    "            true_bbox[2] *= img_width\n",
    "            true_bbox[3] *= img_height\n",
    "            \n",
    "            true_bbox = true_bbox.type(torch.uint8)\n",
    "            true_bbox_converted = box_convert(true_bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "            true_bbox_converted = true_bbox_converted.unsqueeze(0)\n",
    "\n",
    "            img = draw_bounding_boxes(img, true_bbox_converted, colors='lightgreen')\n",
    "\n",
    "        if F.sigmoid(class_pred[idx][0]) > 0.5:\n",
    "            pred_bbox = pred_bboxes[idx]\n",
    "            pred_bbox[0] *= img_width\n",
    "            pred_bbox[1] *= img_height\n",
    "            pred_bbox[2] *= img_width\n",
    "            pred_bbox[3] *= img_height\n",
    "            \n",
    "            pred_bbox = pred_bbox.type(torch.uint8)\n",
    "            pred_bbox_converted = box_convert(pred_bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "            pred_bbox_converted = pred_bbox_converted.unsqueeze(0)\n",
    "\n",
    "            img = draw_bounding_boxes(img, pred_bbox_converted, colors='red')\n",
    "            \n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f'Pred: {int(class_pred[idx][-1])}')\n",
    "        plt.suptitle(f'True label: {label} - Image {start_idx} to {idx}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = predict(best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_test = torch.load('data/localization_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [img for img,_ in loc_test]\n",
    "plot_predictions(imgs, y_true, y_pred, label=3, start_idx=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data and inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.load('data/list_y_true_train.pt')\n",
    "val_labels = torch.load('data/list_y_true_val.pt')\n",
    "test_labels = torch.load('data/list_y_true_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train label size: {len(train_labels)}')\n",
    "print(f'Val label size: {len(val_labels)}')\n",
    "print(f'Test label size: {len(test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_train = torch.load('data/detection_train.pt')\n",
    "det_val = torch.load('data/detection_val.pt')\n",
    "det_test = torch.load('data/detection_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train label size: {len(det_train)}')\n",
    "print(f'Val label size: {len(det_val)}')\n",
    "print(f'Test label size: {len(det_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_to_global(bbox, W_out, H_out, W_img, H_img):\n",
    "    \"\"\"Does things \"\"\"\n",
    "\n",
    "\n",
    "    return bbox\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def global_to_local(global_coordinates, H_out, W_out):\n",
    "    \"\"\"Does things \"\"\"\n",
    "    w_boundaries = [i / W_out for i in range(0, W_out)]\n",
    "    w_boundaries = [(idx, boundary) for idx, boundary in enumerate(w_boundaries)]\n",
    "    w_boundaries.reverse()\n",
    "\n",
    "    h_boundaries = [i / H_out for i in range(0, H_out)]\n",
    "    #h_boundaries = [(idx, boundary) for idx, boundary in enumerate(h_boundaries)]\n",
    "    h_boundaries.reverse()\n",
    "\n",
    "    print(w_boundaries, h_boundaries)\n",
    "\n",
    "    x_global, y_global = global_coordinates[0].item(), global_coordinates[1].item()\n",
    "    x_boundary, x_cell = next(((boundary, idx) for idx, boundary in w_boundaries if boundary < y_global))\n",
    "    y_boundary, y_cell = next(((boundary, idx) for idx, boundary in enumerate(h_boundaries) if boundary < x_global))\n",
    "\n",
    "    x_local = abs((x_global - x_boundary) * W_out)\n",
    "    y_local = abs((y_global - y_boundary) * H_out)\n",
    "\n",
    "    return x_local, y_local, (x_cell, y_cell)\n",
    "    \n",
    "global_cord = [0.5, 0.6771, 0.1667, 0.4792]\n",
    "#print(global_to_local(global_cord, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detection_data(data, H_out, W_out):\n",
    "    \"\"\"W_out H_out should be shape of grid\"\"\"\n",
    "    _, axes = plt.subplots(nrows=2, ncols=5, figsize=(8,3))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat): \n",
    "        \n",
    "        img, label = data[i]\n",
    "        img_height, img_width = img.shape[-2], img.shape[-1]\n",
    "        img = (img * 255).byte()\n",
    "\n",
    "        for row_idx, row in enumerate(label):\n",
    "            for col_idx, gridcell in enumerate(row):\n",
    "                if int(gridcell[0]) == 1:\n",
    "                    bbox = gridcell[1:5].clone()\n",
    "                    \n",
    "                    cell_width = img_width / W_out\n",
    "                    cell_height = img_height / H_out\n",
    "                    x_boundary = col_idx * cell_width\n",
    "                    y_boundary = row_idx * cell_height\n",
    "                    \n",
    "                    bbox[0] = ((bbox[0] / x_boundary) * W_out) * img_width\n",
    "                    bbox[1] = ((bbox[1] / y_boundary) * H_out) * img_height\n",
    "                    bbox[2] = bbox[2] * img_width\n",
    "                    bbox[3] = bbox[3] * img_height\n",
    "\n",
    "                    bbox = bbox.type(torch.uint8)\n",
    "\n",
    "                    converted_bbox = box_convert(bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
    "\n",
    "                    img = draw_bounding_boxes(img, converted_bbox.unsqueeze(0), colors='lightgreen')\n",
    "\n",
    "        img  = img.numpy().transpose((1, 2, 0))\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(i)\n",
    "        ax.axis('off')\n",
    "\n",
    "plot_detection_data(data=det_train, H_out=2, W_out=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.load('data/list_y_true_train.pt')\n",
    "val_labels = torch.load('data/list_y_true_val.pt')\n",
    "test_labels = torch.load('data/list_y_true_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(y_true, H_out, W_out):\n",
    "    \"\"\"\n",
    "    1. Create empty tensor in right format\n",
    "    2. Change to local coordinates\n",
    "    3. Which grid cell does each object belong to?\n",
    "    4. Place in correct cell\n",
    "    \"\"\"\n",
    "    label_tensor = torch.zeros(2, 3, 6)\n",
    "    for label in y_true:\n",
    "        x_local, y_local, grid_pos = global_to_local(label[1:5], H_out, W_out)\n",
    "        print(grid_pos)\n",
    "        label[1] = x_local\n",
    "        label[2] = y_local\n",
    "        label[3] *= W_out\n",
    "        label[4] *= H_out\n",
    "        label_tensor[grid_pos[1], grid_pos[0]] = label\n",
    "    return label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.load('data/list_y_true_train.pt')\n",
    "label = train_labels[1]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_train[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_labels(label, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionLoss(nn.Module):\n",
    "    \"\"\"Custom loss function\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.L_a = nn.BCEWithLogitsLoss()  # detection loss\n",
    "        self.L_b = nn.MSELoss()  # localization loss\n",
    "        self.L_c = nn.CrossEntropyLoss()  # classification loss\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        det_pred = y_pred[:, 0]\n",
    "        bbox_pred = y_pred[:, 1:5]\n",
    "        class_pred = y_pred[:, 5:]\n",
    "\n",
    "        det_true = y_true[:, 0]\n",
    "        bbox_true = y_true[:, 1:5]\n",
    "        class_true = y_true[:, -1].long()\n",
    "\n",
    "        L_a = self.L_a(det_pred, det_true)\n",
    "\n",
    "        object_detected = det_true == 1\n",
    "\n",
    "        L_b = self.L_b(bbox_pred[object_detected], bbox_true[object_detected])\n",
    "        L_c = self.L_c(class_pred[object_detected], class_true[object_detected])\n",
    "\n",
    "        return L_a + L_b + L_c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
